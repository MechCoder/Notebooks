{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.contrib import learn\n",
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(X_train, y_train, num_epochs=5, batch_size=32, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    dataset_size = X_train.shape[0]\n",
    "    indices = np.arange(dataset_size)\n",
    "    start_indices = np.arange(0, dataset_size, batch_size)\n",
    "    end_indices = np.arange(batch_size, dataset_size, batch_size)\n",
    "    if len(start_indices) != len(end_indices):\n",
    "        start_indices = start_indices[:-1]\n",
    "    for i in range(num_epochs):\n",
    "        rng.shuffle(indices)\n",
    "        X_train = X_train[indices, :]\n",
    "        y_train = y_train[indices]\n",
    "        for start_ind, stop_ind in zip(start_indices, end_indices):\n",
    "            yield X_train[start_ind: stop_ind], y_train[start_ind: stop_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPModel(object):\n",
    "    \"\"\"\n",
    "    Tensorflow 2-layer MLP that allows setting learning rate and regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size=32, n_features=100, n_classes=10):\n",
    "        self.X = tf.placeholder(tf.float32, [batch_size, n_features])\n",
    "        self.y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = tf.placeholder(tf.float32)\n",
    "        self.reg = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Layer 1: 256 hidden units.\n",
    "        W1 = tf.get_variable(\n",
    "            \"Weights1\", shape=[n_features, 256], dtype=tf.float32)\n",
    "        b1 = tf.get_variable(\"Bias1\", shape=[256], dtype=tf.float32)\n",
    "        X_hidden1 = tf.add(tf.matmul(self.X, W1), b1)\n",
    "        X_hidden1 = tf.nn.relu(X_hidden1)\n",
    "\n",
    "        # Layer 2: 10 classes\n",
    "        W2 = tf.get_variable(\n",
    "            \"Weights2\", shape=[256, n_classes], dtype=tf.float32)\n",
    "        b2 = tf.get_variable(\"Bias2\", shape=[n_classes], dtype=tf.float32)\n",
    "        X_hidden2 = tf.add(tf.matmul(X_hidden1, W2), b2)\n",
    "\n",
    "        l2_loss = tf.nn.l2_loss(W1)\n",
    "        l2_loss += tf.nn.l2_loss(b1)\n",
    "        l2_loss += tf.nn.l2_loss(W2)\n",
    "        l2_loss += tf.nn.l2_loss(W2)\n",
    "\n",
    "        self.loss_ = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                X_hidden2, self.y)) + self.reg * l2_loss\n",
    "        predictions = tf.argmax(X_hidden2, 1)\n",
    "        true = tf.argmax(self.y, 1)\n",
    "        self.accuracy_ = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(predictions, true), tf.float32))\n",
    "\n",
    "        self.optimizer_ = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.learning_rate).minimize(self.loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_2layer_mlp(params):\n",
    "    init_scale, learning_rate, reg = params\n",
    "\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    session = tf.Session()\n",
    "\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "    # Generate random scopes for every function call to prevent reusing weights\n",
    "    # from previous function calls.\n",
    "    random_scope = \"MLPModel\" + str(rng.randn())\n",
    "    with tf.variable_scope(random_scope, reuse=None, initializer=initializer):\n",
    "        mlpmodel = MLPModel(n_features=X_train.shape[1])\n",
    "\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    for X_batch, y_batch in batch_iter(X_train, y_train):\n",
    "        feed_dict = {\n",
    "            mlpmodel.X: X_batch,\n",
    "            mlpmodel.y: y_batch,\n",
    "            mlpmodel.learning_rate: learning_rate,\n",
    "            mlpmodel.reg: reg}\n",
    "        ops = {\n",
    "            \"optimizer\": mlpmodel.optimizer_,\n",
    "            \"accuracy\": mlpmodel.accuracy_,\n",
    "        }\n",
    "        vals = session.run(ops, feed_dict)\n",
    "\n",
    "    with tf.variable_scope(random_scope, reuse=True):\n",
    "        mlpmodelval = MLPModel(\n",
    "            batch_size=X_test.shape[0], n_features=X_train.shape[1])\n",
    "\n",
    "    # In practise, one should use a validation set independent of the test set.\n",
    "    feed_dict[mlpmodelval.X] = X_test\n",
    "    feed_dict[mlpmodelval.y] = y_test\n",
    "    ops = {\"accuracy\": mlpmodel.accuracy_}\n",
    "    vals = session.run(ops, feed_dict)\n",
    "    return -vals['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "digits = learn.datasets.mnist.load_mnist()\n",
    "X_train, y_train = digits.train.images, digits.train.labels\n",
    "X_test, y_test = digits.test.images, digits.test.labels\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(np.unique(y_train))\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Iteration No: 1 ended. Evaluation done at random point."
     ]
    }
   ],
   "source": [
    "bounds = [(0.0, 0.5), (1e-6, 1e-1, \"log-uniform\"), (1e-3, 1, \"log-uniform\")]\n",
    "res = gp_minimize(optimize_2layer_mlp, bounds, n_calls=20, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
